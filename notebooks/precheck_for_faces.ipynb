{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26ecfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b155d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_duration(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = int(math.ceil(frame_count/fps))\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c727f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner/outer lips landmark indices (MediaPipe FaceMesh, 468 points).\n",
    "# We'll compute mouth opening from MANY points to reduce noise.\n",
    "# Sources summarizing lip indices and FaceMesh basics: MediaPipe docs + community maps.\n",
    "UPPER_LIPS = [13, 82, 81, 42, 183, 78]      # upper inner/near-inner band (includes mid 13)\n",
    "LOWER_LIPS = [14, 87, 178, 88, 95]          # lower inner/near-inner band (includes mid 14)\n",
    "MOUTH_CORNERS_INNER = (78, 308)             # inner corners (more stable for width than 61/291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b821c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _xy_from_landmarks(landmarks, w, h, idx):\n",
    "    lm = landmarks[idx]\n",
    "    return np.array([lm.x * w, lm.y * h], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ee8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mouth_width(landmarks, w, h):\n",
    "    L = _xy_from_landmarks(landmarks, w, h, MOUTH_CORNERS_INNER[0])\n",
    "    R = _xy_from_landmarks(landmarks, w, h, MOUTH_CORNERS_INNER[1])\n",
    "    return float(np.linalg.norm(R - L) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce9325e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _aperture_from_many_pairs(landmarks, w, h):\n",
    "    \"\"\"\n",
    "    Robust mouth opening:\n",
    "    - Builds sets of upper/lower lip points.\n",
    "    - Pairs each upper point to the closest-by-x lower point.\n",
    "    - Takes the median vertical gap and normalize by inner-corner width.\n",
    "    \"\"\"\n",
    "    width = _mouth_width(landmarks, w, h)\n",
    "    if width <= 1e-6:\n",
    "        return 0.0\n",
    "\n",
    "    upp = np.array([_xy_from_landmarks(landmarks, w, h, i) for i in UPPER_LIPS])\n",
    "    low = np.array([_xy_from_landmarks(landmarks, w, h, i) for i in LOWER_LIPS])\n",
    "\n",
    "    # Pair by nearest x (prevents relying on exact index correspondences)\n",
    "    gaps = []\n",
    "    for u in upp:\n",
    "        j = np.argmin(np.abs(low[:,0] - u[0]))  # nearest x\n",
    "        vgap = abs(low[j,1] - u[1])            # vertical distance\n",
    "        gaps.append(vgap)\n",
    "    if not gaps:\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.median(gaps) / width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c093cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precheck_video_for_speaking(\n",
    "    video_path: str,            \n",
    "    sample_fps: float = 5.0,           # sample rate for the precheck\n",
    "    min_face_fraction: float = 0.2,   # require faces in at least 35% of sampled frames\n",
    "    min_modulation_std: float = 0.015, # require some variance over time\n",
    "    adapt_k: float = 0.6,              # how far above baseline we call it \"open\"\n",
    "    min_open_fraction: float = 0.20    # require at least 30% of face frames to be “open”  -> i.e. person is talking\n",
    "):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return dict(ok=False, reason=f\"Could not open {video_path}\")\n",
    "    \n",
    "    #video_duration = _get_duration(video_path)\n",
    "    video_duration = int(45)\n",
    "\n",
    "    native_fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    total_frames_to_scan = int(min(video_duration * native_fps, cap.get(cv2.CAP_PROP_FRAME_COUNT) or 1e9))\n",
    "    step = max(int(round(native_fps / sample_fps)), 1)\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,         # << better lip detail\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "    )\n",
    "\n",
    "    sampled, face_frames = 0, 0\n",
    "    apertures = []\n",
    "    frame_idx = 0\n",
    "    while frame_idx < total_frames_to_scan:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        h, w = frame.shape[:2]\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = face_mesh.process(rgb)\n",
    "        sampled += 1\n",
    "\n",
    "        if res.multi_face_landmarks:\n",
    "            face_frames += 1\n",
    "            lms = res.multi_face_landmarks[0].landmark\n",
    "            apertures.append(_aperture_from_many_pairs(lms, w, h))\n",
    "\n",
    "        frame_idx += step\n",
    "\n",
    "    cap.release()\n",
    "    try:\n",
    "        face_mesh.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if sampled == 0:\n",
    "        return dict(ok=False, reason=\"No frames sampled\")\n",
    "\n",
    "    face_fraction = face_frames / sampled\n",
    "    if face_fraction < min_face_fraction:\n",
    "        return dict(\n",
    "            ok=False,\n",
    "            reason=f\"Face too infrequent: {face_fraction:.2f} < {min_face_fraction}\",\n",
    "            face_fraction=face_fraction\n",
    "        )\n",
    "\n",
    "    if len(apertures) < 5:\n",
    "        return dict(ok=False, reason=\"Too few lip samples\", face_fraction=face_fraction)\n",
    "\n",
    "    a = np.array(apertures, dtype=np.float32)\n",
    "    # Smooth slightly to suppress per-frame jitter\n",
    "    if len(a) >= 5:\n",
    "        a = np.convolve(a, np.ones(5)/5.0, mode=\"same\")\n",
    "\n",
    "    # Adaptive threshold: baseline = low quantile; high = 90th quantile\n",
    "    base = float(np.quantile(a, 0.20))   # ~closed mouth level\n",
    "    hi   = float(np.quantile(a, 0.90))   # very open\n",
    "    thr  = base + adapt_k * max(hi - base, 1e-6)\n",
    "\n",
    "    open_ratio = float(np.mean(a > thr))\n",
    "    a_std = float(a.std())\n",
    "\n",
    "    talking_like = (a_std >= min_modulation_std) and (open_ratio >= min_open_fraction)\n",
    "    return dict(\n",
    "        ok=talking_like,\n",
    "        reason=(\"Face present and speech-like mouth motion\" if talking_like\n",
    "                else f\"Not speech-like: open_ratio={open_ratio:.2f} (min {min_open_fraction}), std={a_std:.3f} (min {min_modulation_std})\"),\n",
    "        face_fraction=face_fraction,\n",
    "        open_ratio=open_ratio,\n",
    "        aperture_std=a_std,\n",
    "        thr=thr,\n",
    "        stats=dict(min=float(a.min()), med=float(np.median(a)), p90=hi, base=base)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27adf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757183794.514027   44331 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1757183794.529857   44490 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.1), renderer: llvmpipe (LLVM 19.1.1, 256 bits)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1757183794.533856   44433 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757183794.552621   44435 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757183794.597387   44433 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "video_path = \"/home/ssever/SilentSpeak/data/input_video/How To Talk To Camera_ The 3 FUNDAMENTALS.mp4\"\n",
    "\n",
    "check = precheck_video_for_speaking(video_path)\n",
    "check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
